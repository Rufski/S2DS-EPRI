{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "This notebook presents a typical use of neural network to time series of the EPRI project. A lot of ad hoc modifications are possible, especially at the stage of loading the data, as this is the moment where data is preprocessed and the input and output are determined. When moving the code to an implementable .py file **one should rewrite preprocessing into a pipeline, so that raw data could be fed to the machine.**\n",
    "\n",
    "Essential parts that have to be edited are preceded by a comment starting with \"# TO EDIT\". Optional steps are preceded by a comment that ends with \"optional.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import os\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "import math\n",
    "from tensorflow.keras import initializers\n",
    "import sys\n",
    "\n",
    "# TO EDIT: replace path with your own path to Aug20_Epri/src/data\n",
    "# But only needed if you want to use any function from make_dataset.py\n",
    "sys.path.insert(1, '/Users/antoine/ml/S2DS/EPRI/Aug20_Epri/src/data')\n",
    "\n",
    "from make_dataset import remove_clipping_with_flexible_window as rcwfw\n",
    "from make_dataset import remove_clipping_with_universal_window as rcwuw\n",
    "import datetime\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and preparing the data for the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating datasets of 5-year-long and 3-year-long series\n",
    "# X being the input, Y being the target\n",
    "# TO NOTE: 3-year-long time series are picked from the 5-year-long\n",
    "# as series starting from a 1st of January and ending on a 31st of December\n",
    "X_3years = np.empty((0, 365*3), float)\n",
    "X_5years = np.empty((0, 365*5), float)\n",
    "Y_3years = np.empty((0, 365*3), float)\n",
    "Y_5years = np.empty((0, 365*5), float)\n",
    "\n",
    "# List of data to import; feel free to exclude data you don't need by commenting it out\n",
    "datasets_addresses = [\n",
    "    \"synthetic_basic/synthetic_basic\",\n",
    "#    \"synthetic_soil/synthetic_soil\",\n",
    "#    \"synthetic_weather/synthetic_weather\",\n",
    "#    \"synthetic_soil_weather/synthetic_soil_weather\",\n",
    "#    \"synthetic_soil_weather_locations/synthetic_soil_weather_locations\"\n",
    "]\n",
    "\n",
    "for dataset in datasets_addresses:\n",
    "# Careful here: datasets have different lengths; the range() has to be adjusted and\n",
    "# might be problematic if you want to load both the 50 basic series and the 150\n",
    "# soil and weather series \n",
    "    for i in range(1, 51):\n",
    "        print(\"Loading file #\"+str(i)+\" from dataset \"+dataset.split(\"/\")[0])\n",
    "            \n",
    "        # Loading a time series\n",
    "        index = str(i)\n",
    "        if i < 10:\n",
    "            index = \"00\"+str(i)\n",
    "        elif i <100:\n",
    "            index = \"0\"+str(i)\n",
    "        df = pd.read_pickle(\"./data/raw/\"+dataset+\"_\"+index+\".pkl\", \"gzip\")\n",
    "        \n",
    "        # TO EDIT: the preprocessing happens here, to be edited depending on the desired outcome\n",
    "        df[\"date_and_hour\"] = [str(i)+\"-\"+str(j) for i,j in zip(df.index.date,df.index.hour)]\n",
    "        df = df[df.date_and_hour.str[5:10]!=\"02-29\"]\n",
    "        power_df = df[df.POA > 1.0].copy()\n",
    "        power_df[\"Power_norm\"] = (power_df.Power+1) / power_df.POA\n",
    "        power_df = power_df.groupby(power_df.index.date, as_index=False).Power_norm.median()\n",
    "\n",
    "        # TO EDIT: adding target to the target list. In this example I'm creating a time series\n",
    "        # of the degradation with daily resolution, taking for each day the value of the degradation\n",
    "        # at the first minute of the day.\n",
    "        Y_5years = np.vstack([\n",
    "            Y_5years,\n",
    "            np.array(df[df.index.time == datetime.time(0,0)].Degradation)])\n",
    "        for j in range(0,3):\n",
    "            Y_3years = np.vstack([\n",
    "                Y_3years,\n",
    "                np.array(df[df.index.time == datetime.time(0,0)].Degradation[:365*3])])\n",
    "        \n",
    "        # TO EDIT: adding input to the input list. In this example I add a series of the\n",
    "        # normalized power Power_norm computed earlier\n",
    "        X_5years = np.vstack([\n",
    "            X_5years,\n",
    "            np.array(power_df.Power_norm)\n",
    "        ])\n",
    "        for j in range(0,3):\n",
    "            X_3years = np.vstack([\n",
    "                X_3years,\n",
    "                np.array(power_df.Power_norm[365*j:365*(j+3)])\n",
    "            ])\n",
    "\n",
    "print(\"All done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the sample set & target you wanna work on\n",
    "\n",
    "X = X_3years\n",
    "Y = Y_3years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the data looks like what you expect. Optional but recommended.\n",
    "\n",
    "plt.figure(figsize=(15,20))\n",
    "plt.suptitle(\"Examples of the loaded data: input\")\n",
    "for i in range(0, 10):\n",
    "    plt.xlabel(\"Time steps\")\n",
    "    plt.ylabel(\"Power\")\n",
    "    plt.subplot(5,2,i+1)\n",
    "    plt.plot(X[i])\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Examples of the loaded data: target\")\n",
    "plt.xlabel(\"Time steps\")\n",
    "plt.ylabel(\"Normalized degradation factor\")\n",
    "for i in range(0, 10):\n",
    "    plt.plot(Y[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividing the data into training, validation and test sets.\n",
    "\n",
    "# TO EDIT: if you want a proportion of train/valid/test that\n",
    "# is different from 70%-15%-15%\n",
    "train_index = int(len(X)*0.7)\n",
    "valid_index = int(len(X)*0.85)\n",
    "\n",
    "X_train = X[:train_index]\n",
    "X_valid = X[train_index:valid_index]\n",
    "X_test = X[valid_index:]\n",
    "Y_train = Y[:train_index]\n",
    "Y_valid = Y[train_index:valid_index]\n",
    "Y_test = Y[valid_index:]\n",
    "\n",
    "# Shuffling the datasets so that, in the case of 3-year-long data,\n",
    "# mini-batch training doesn't get a streak of samples that were\n",
    "# all extracted from the same time series.\n",
    "train_shuffle = np.random.permutation(len(X_train))\n",
    "valid_shuffle = np.random.permutation(len(X_valid))\n",
    "\n",
    "X_train = X_train[train_shuffle]\n",
    "Y_train = Y_train[train_shuffle]\n",
    "X_valid = X_valid[valid_shuffle]\n",
    "Y_valid = Y_valid[valid_shuffle]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For tensorboard and model naming.\n",
    "run_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping stops training when the loss of the validation set does not beat\n",
    "# the previous minimum after a certain number of training epochs.\n",
    "# Number of epochs after which to stop is determined by the parameter\n",
    "# \"patience.\" Optional, but then remove it from the callbacks (see last line of the cell).\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=300)\n",
    "\n",
    "# To update the name of the model, indicating this is the nth run\n",
    "run_index = run_index + 1\n",
    "\n",
    "# TO EDIT: to allow you to distinguish between models you're training. Will be used\n",
    "# when saving the model and outputing training logs.\n",
    "model_name = \"my_pretty_little_machine\"\n",
    "\n",
    "# Creates and sends log to tensorboard. Optional but useful.\n",
    "run_logdir = os.path.join(os.curdir, model_name+\"_log\", \"run_{:03d}\".format(run_index))\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "\n",
    "# To save the best model so far during training.\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(model_name+\"_run_\"+str(run_index)+\".h5\", save_best_only=True)\n",
    "\n",
    "time_series_length = len(X[0])\n",
    "\n",
    "# Creating and compiling the model. Have fun!\n",
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(keras.layers.Input(shape=[time_series_length]))\n",
    "\n",
    "for i in range(0, 6):\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Dense(time_series_length, activation=\"elu\", kernel_initializer=\"he_normal\"))\n",
    "\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Dense(time_series_length, activation=\"sigmoid\", kernel_initializer=\"he_normal\"))\n",
    "\n",
    "model.compile(loss='mse', optimizer=keras.optimizers.Adam(clipvalue=0.5))\n",
    "\n",
    "# Training time!\n",
    "history = model.fit(X_train,\n",
    "                    Y_train,\n",
    "                    epochs=100,\n",
    "                    validation_data=(X_valid, Y_valid),\n",
    "                    callbacks=[checkpoint_cb, early_stopping_cb, tensorboard_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the best parameters and generating a list of predictions.\n",
    "model = keras.models.load_model(model_name+\"_run_\"+str(run_index)+\".h5\")\n",
    "prediction = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the predictions.\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.tight_layout()\n",
    "plt.suptitle(\"Normalized decay factor vs time step\")\n",
    "for i in range(0, min(len(prediction), 9)):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.ylim(0.94,1.01)\n",
    "    plt.plot([x for x in prediction[i]])\n",
    "    plt.plot([x for x in Y_test[i]])\n",
    "plt.legend(['NN prediction','Actual decay'])\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,3))\n",
    "plt.suptitle(\"Normalized decay factor vs timestep: predicted (left) vs target (right)\")\n",
    "plt.subplot(1,2,1)\n",
    "plt.ylim(0.90,1)\n",
    "for i in range(0,len(prediction)):\n",
    "    plt.plot(prediction[i])\n",
    "plt.subplot(1,2,2)\n",
    "plt.ylim(0.90,1)\n",
    "for i in range(0,len(Y_test)):\n",
    "    plt.plot(Y_test[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To convert a decay series into a decay rate.\n",
    "def extract_yr_degrad(avg_power_decay):\n",
    "    x = np.array([i/365 for i in range(0,len(avg_power_decay))])\n",
    "    y = np.array(avg_power_decay)\n",
    "    return np.polyfit(x,y,1)[0]\n",
    "\n",
    "# Computing the decay rates from the prediction and target, and\n",
    "# computing the RMSE between those.\n",
    "model_pred = []\n",
    "degradation_rates_test = []\n",
    "for i in range(0,len(prediction)):\n",
    "    model_pred.append(extract_yr_degrad(prediction[i,-400:]))\n",
    "    degradation_rates_test.append(extract_yr_degrad(Y_test[i]))\n",
    "    \n",
    "mse = metrics.mean_squared_error(model_pred, degradation_rates_test)\n",
    "math.sqrt(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting decay rate (predicted vs target)\n",
    "\n",
    "plt.figure(figsize=(13,3))\n",
    "plt.plot(degradation_rates_test[2:50])\n",
    "plt.plot(model_pred[2:50])\n",
    "plt.xlabel(\"Time series sample #\")\n",
    "plt.ylabel(\"Yearly decay rate\")\n",
    "plt.title(\"NN prediction vs actual yearly decay rate\")\n",
    "plt.legend([\"Actual\",\"NN predicted\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case you used the 3-year-long samples: this takes the mean of three predictions\n",
    "# of 3-year-long samples extracted from the same 5-year-long time series, thus\n",
    "# computing an ensemble prediction. Sometimes fares better than simple prediction.\n",
    "\n",
    "model_pred_aggr = []\n",
    "degrad_rate_aggr = []\n",
    "for i in range(len(model_pred)%3,len(model_pred),3):\n",
    "    degrad_rate_aggr.append(degradation_rates_test[i])\n",
    "    model_pred_aggr.append(np.mean(model_pred[i:i+3]))\n",
    "    \n",
    "plt.figure(figsize=(13,3))\n",
    "plt.plot(degrad_rate_aggr)\n",
    "plt.plot(model_pred_aggr)\n",
    "plt.xlabel(\"Time series sample #\")\n",
    "plt.ylabel(\"Yearly decay rate\")\n",
    "plt.title(\"NN prediction vs actual yearly decay rate\")\n",
    "plt.legend([\"Actual\",\"NN predicted\"])\n",
    "plt.show()\n",
    "\n",
    "mse = metrics.mean_squared_error([-0.01]*len(degrad_rate_aggr), degrad_rate_aggr)\n",
    "math.sqrt(mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section is left empty for you to copy/paste models and preprocessing that works well. Too often did I want to try a modification and ended up editing and then forgetting about the edits of a promising model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
