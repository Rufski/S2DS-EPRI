{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "This notebook presents a typical use of neural network to time series of the EPRI project.\n",
    "\n",
    "It is strongly suggested to read the Neural Network report provided in the Report folder of the shared box in order to understand the various pre-processing methods we refer to.\n",
    "\n",
    "Choice was made to leave several possibly conflicting lines of code in the cells; instructions were left to indicate when commenting out lines was desirable.\n",
    "\n",
    "A lot of ad hoc modifications are possible, especially at the stage of loading the data, as this is the moment where data is preprocessed and the input and output are determined. When moving the code to an implementable .py file **one should rewrite preprocessing into a pipeline, so that raw data could be fed to the machine.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to reproduce results from the Neural Network report:\n",
    "\n",
    "#### Basic dataset\n",
    "\n",
    "In order to reproduce the results in the report, follow this procedure:\n",
    "\n",
    "1) *No subsampling.* For each of the 'choose one of two' mentions in part 2, comment out b). Make sure the second cell of part 2 loads `X_5years` and `Y_5years`.\n",
    "\n",
    "2) *Downsizing to day-by-day series using the 70th percentile of power for each day.* In part 2.III.iv, edit the tail of the `groupby` line to be `.quantile(q=0.7)`\n",
    "\n",
    "3) *No other preprocessing.* Comment out part 2.III.iii\n",
    "\n",
    "4) Make sure that the downsizing (part 2.III.iv) processes the `Power` (as opposed to `Power_norm` or `Power_scaled`).\n",
    "\n",
    "5) *Model naming.* Part 3.III, rename the model to your convenience.\n",
    "\n",
    "Training converges to optimal solution within ~500 epochs. Training can be interrupted midway by interrupting the jupyter notebook kernel; best model found during training will still be saved.\n",
    "\n",
    "#### Soiling, Weather, Soil and Weather datasets\n",
    "\n",
    "2-year subsampling performing consistently worse than 3-year subsampling and no subsampling, while 4-year subsampling never outperformed 3-year subsampling. Thus we decided not to include these other subsampling methods in this jupyter notebook. If needed it shouldn't be too hard to take example on the 3-year subsampling coded here and adapt it for 2-year or 4-year subsampling.\n",
    "\n",
    "The various daily aggregations pre-processing can be tested by modifying the tail of the `groupby` line in part 2.III.iv accordingly:\n",
    "- `.quantile(q=0.7)` for the 70th percentile of power for each day\n",
    "- `.median()` for the median power for each day\n",
    "- `.mean()` for the mean power for each day\n",
    "\n",
    "If you want to use a power clipping function, simply adapt part 2.III.iii by commenting out all but power clipping lines.\n",
    "\n",
    "#### Warning about optional preprocessing (part 2.III.iii and following)\n",
    "\n",
    "Most of the preprocessing methods create a new column in the dataframe each time preprocessing is applied. If you want to go through several preprocessing steps (say, scaling, then POA-normalization) make sure you edit the code accordingly so that each step's input corresponds to the output of the previous step (in the example above: scaling takes `df.Power` as input, and produces `df.Power_scaled`; then, normalization takes `df.Power_scaled` as input and produces `df.Power_normalized`). Also, make sure that part 2.III.iv takes as input the final output of the hitherto preprocessing (in the above example, `df.Power_normalized`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Imports and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import os\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "import math\n",
    "from tensorflow.keras import initializers\n",
    "import sys\n",
    "from tensorflow.keras import backend as K\n",
    "import datetime\n",
    "\n",
    "from src.data.make_dataset import remove_clipping_with_flexible_window as remove_clip_flex\n",
    "from src.data.make_dataset import remove_clipping_with_universal_window as remove_clip_univ\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting a decay series into a decay rate.\n",
    "def extract_yr_degrad(avg_power_decay):\n",
    "    x = np.array([i/365 for i in range(0,len(avg_power_decay))])\n",
    "    y = np.array(avg_power_decay)\n",
    "    return np.polyfit(x,y,1)[0]\n",
    "\n",
    "\n",
    "# Our personal activation function for the last layer of the neural network\n",
    "def paf(x):\n",
    "    return 0.21*K.sigmoid(x)+0.8\n",
    "tf.keras.utils.get_custom_objects().update({'paf': paf})\n",
    "\n",
    "\n",
    "# Generating noise to test normalization by irradiance with sensor drift\n",
    "# Parameters A1 and K1 can be adjusted\n",
    "def generate_noise(series_length):\n",
    "    freq = np.fft.rfftfreq(series_length)\n",
    "    sp = []\n",
    "    A1 = 0.002\n",
    "    K1 = 1-1e-3\n",
    "    for freq_iter in np.nditer(freq):\n",
    "        if freq_iter == 0:\n",
    "            sp.append(0)\n",
    "        else:\n",
    "            std_iter = A1*freq_iter**(-K1)\n",
    "            sp_real = np.random.normal(0, std_iter)\n",
    "            sp_imag = np.random.normal(0, std_iter)\n",
    "            sp.append(sp_real + 1j*sp_imag)\n",
    "    sp = np.asarray(sp)\n",
    "    return np.fft.irfft(sp)+1\n",
    "\n",
    "\n",
    "# Smoothing a power series by taking a rolling median of the series.\n",
    "# window is the size of the subarray over which median is taken.\n",
    "# step is the number of steps by which the median window is shifted\n",
    "# to compute the next median.\n",
    "def rolling_median(s, window, step):\n",
    "    s = pd.Series([0]*(window//2)+list(s)+[0]*(window//2+window%2))\n",
    "    vert_idx_list = np.arange(0, s.size - window, step)\n",
    "    hori_idx_list = np.arange(window)\n",
    "    A, B = np.meshgrid(hori_idx_list, vert_idx_list)\n",
    "    idx_array = A + B\n",
    "    x_array = s.values[idx_array]\n",
    "    idx = s.index[vert_idx_list + int(window/2.)]\n",
    "    d = np.quantile(x_array,0.5, axis=1)\n",
    "    return np.array(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Loading and pre-processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# I. Creating datasets of 5-year-long and 3-year-long series\n",
    "#    X being the input, Y being the target\n",
    "\n",
    "# Choose one of two (comment out the choice not needed)\n",
    "\n",
    "# a) 5-year-long input (no subsampling)\n",
    "X_5years = np.empty((0, 365*5+1), float)\n",
    "Y_5years = np.empty((0, 365*5+1), float)\n",
    "\n",
    "# b) 3-year-long input (using subsampling)\n",
    "X_3years = np.empty((0, 365*3), float)\n",
    "Y_3years = np.empty((0, 365*3), float)\n",
    "\n",
    "\n",
    "# II. Choosing the dataset. Comment out datasets you don't need.\n",
    "# The following assumes that the zip of pkl has been extracted,\n",
    "# and all the pkl files are in data/raw/synthetic_<name_of_datastage>\n",
    "datasets_addresses = [\n",
    "    \"synthetic_basic/synthetic_basic\",\n",
    "    \"synthetic_soil/synthetic_soil\",\n",
    "    \"synthetic_weather/synthetic_weather\",\n",
    "    \"synthetic_soil_weather/synthetic_soil_weather\",\n",
    "    \"synthetic_soil_weather_locations/synthetic_soil_weather_locations\"\n",
    "]\n",
    "\n",
    "# III. Loading, preprocessing and adding each time series to input and target arrays\n",
    "\n",
    "for dataset in datasets_addresses:\n",
    "    \n",
    "    files_count = len(\n",
    "        [f for f in os.listdir(\"../data/raw/\"+dataset.split(\"/\")[0])\n",
    "         if f.endswith('.pkl')\n",
    "         and f.startswith(dataset.split(\"/\")[1]+\"_\")\n",
    "         and os.path.isfile(os.path.join(\"../data/raw/\"+dataset.split(\"/\")[0], f))]\n",
    "    )\n",
    "\n",
    "    for i in range(1, files_count+1):\n",
    "        print(\"Loading file #\"+str(i)+\" from dataset \"+dataset.split(\"/\")[0])\n",
    "            \n",
    "        # i. Loading a time series\n",
    "        index = str(i)\n",
    "        if i < 10:\n",
    "            index = \"00\"+str(i)\n",
    "        elif i <100:\n",
    "            index = \"0\"+str(i)\n",
    "        df = pd.read_pickle(\"../data/raw/\"+dataset+\"_\"+index+\".pkl\", \"gzip\")\n",
    "        \n",
    "        # ii. Adding target to the target list, taking for each day the value of the\n",
    "        #     degradation at the first minute of the day.\n",
    "        \n",
    "        # Choose one of two:\n",
    "        \n",
    "        # a) For 5-year-long input (no subsampling)\n",
    "        Y_5years = np.vstack([\n",
    "            Y_5years,\n",
    "            np.array(df[df.index.time == datetime.time(0,0)].Degradation)])\n",
    "    \n",
    "        # b) For 3-year-long input (subsampling)\n",
    "        for j in range(0,3):\n",
    "            Y_3years = np.vstack([\n",
    "                Y_3years,\n",
    "                np.array(df[df.index.time == datetime.time(0,0)].Degradation[:365*3])])\n",
    "\n",
    "        # iii. Optional preprocessing. Comment out any that is not needed.\n",
    "        \n",
    "        # Applying power clipping removal\n",
    "        df[\"minute_of_day\"] = df.index.hour*60+df.index.minute\n",
    "        df = remove_clip_flex(df)\n",
    "        \n",
    "        # Scaling the power signal\n",
    "        df[\"Power_scaled\"] = (df.Power+1)/df.Power.max()\n",
    "        \n",
    "        # Normalizing by irradiance with sensor drift\n",
    "        drift_noise = generate_noise(len(df) + 1) \n",
    "        # generate_noise(n) returns an array of length n-1 if n is odd, so len(df)+1\n",
    "        # to make sure that an array generated is long enough\n",
    "        df[\"Drifting_POA\"] = df.POA * drift_noise[:len(df)]\n",
    "        df = df[df.Drifting_POA > 1.0]\n",
    "        df[\"Power_norm\"] = df.Power_scaled / df.Drifting_POA\n",
    "        \n",
    "        # iv. Downsizing to day-by-day time series.\n",
    "        #     The function at the tail of the groupby line can be edited to change\n",
    "        #     the downsizing method.\n",
    "        \n",
    "        preprocessed_power = np.array(\n",
    "            df.groupby(df.index.date, as_index=False).Power_scaled.quantile(q=0.7)\n",
    "        ).reshape(365*5+1)\n",
    "        \n",
    "        # Optional: smoothing the power signal with a rolling median or savgol filter.\n",
    "        # Usually one of the two lines suffice.\n",
    "        # Function parameters can be adapted at will.\n",
    "        \n",
    "        preprocessed_power = rolling_median(preprocessed_power, 30, 1)\n",
    "        # preprocessed_power = savgol_filter(preprocessed_power, 30, 2)\n",
    "        \n",
    "        # Choose one of two:\n",
    "        \n",
    "        # a) For 5-year-long input (no subsampling)\n",
    "        X_5years = np.vstack([\n",
    "            X_5years,\n",
    "            preprocessed_power\n",
    "        ])\n",
    "        \n",
    "        # b) For 3-year-long input (subsampling)\n",
    "        for j in range(0,3):\n",
    "            X_3years = np.vstack([\n",
    "                X_3years,\n",
    "                preprocessed_power[365*j:365*(j+3)]\n",
    "            ])\n",
    "\n",
    "print(\"All done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IV. Choose the sample set & target you wanna train on.\n",
    "#     This step is added in case you want to load\n",
    "#     BOTH 3-year-long AND 5-year-long input.\n",
    "\n",
    "X = X_3years\n",
    "Y = Y_3years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V. Optional: check that the data looks like what you expect.\n",
    "\n",
    "plt.figure(figsize=(15,20))\n",
    "plt.suptitle(\"Examples of the loaded data: input\")\n",
    "for i in range(0, 10):\n",
    "    plt.xlabel(\"Time steps\")\n",
    "    plt.ylabel(\"Power\")\n",
    "    plt.subplot(5,2,i+1)\n",
    "    plt.plot(X[i])\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Examples of the loaded data: target\")\n",
    "plt.xlabel(\"Time steps\")\n",
    "plt.ylabel(\"Normalized degradation factor\")\n",
    "for i in range(0, 10):\n",
    "    plt.plot(Y[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Training the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I. Dividing the data into training, validation and test sets,\n",
    "#    in proportion 75%-15%-15%\n",
    "\n",
    "train_index = int(len(X)*0.7)\n",
    "valid_index = int(len(X)*0.85)\n",
    "\n",
    "X_train = X[:train_index]\n",
    "X_valid = X[train_index:valid_index]\n",
    "X_test = X[valid_index:]\n",
    "Y_train = Y[:train_index]\n",
    "Y_valid = Y[train_index:valid_index]\n",
    "Y_test = Y[valid_index:]\n",
    "\n",
    "# II. Shuffling the datasets so that, in the case of 3-year-long data,\n",
    "#     mini-batch training doesn't get a streak of samples that were\n",
    "#     all extracted from the same time series.\n",
    "train_shuffle = np.random.permutation(len(X_train))\n",
    "valid_shuffle = np.random.permutation(len(X_valid))\n",
    "\n",
    "X_train = X_train[train_shuffle]\n",
    "Y_train = Y_train[train_shuffle]\n",
    "X_valid = X_valid[valid_shuffle]\n",
    "Y_valid = Y_valid[valid_shuffle]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# III. Initializing parameters necessary for training, and creating a folder\n",
    "#      to store the model's log and results\n",
    "run_index = 0\n",
    "model_name = \"my_nn_model\"\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "final_directory = os.path.join(current_directory, model_name)\n",
    "if not os.path.exists(final_directory):\n",
    "    os.makedirs(final_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IV. Training the neural network\n",
    "\n",
    "# i. To update the name of the model, indicating this is the nth run\n",
    "run_index = run_index + 1\n",
    "\n",
    "# ii. Optional: creating and sending log to tensorboard.\n",
    "run_logdir = os.path.join(os.curdir, model_name+\"_log\", \"run_{:03d}\".format(run_index))\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "\n",
    "# iii. Saving the best model so far during training.\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(model_name+\"_run_\"+str(run_index)+\".h5\", save_best_only=True)\n",
    "\n",
    "time_series_length = len(X[0])\n",
    "\n",
    "# iv. Creating and compiling the model.\n",
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(keras.layers.Input(shape=[time_series_length]))\n",
    "\n",
    "for i in range(0, 6):\n",
    "    model.add(\n",
    "        keras.layers.Dense(\n",
    "            time_series_length,\n",
    "            activation=\"elu\",\n",
    "            kernel_initializer=\"he_normal\"\n",
    "        ))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    \n",
    "model.add(\n",
    "    keras.layers.Dense(\n",
    "        len(Y[0]),\n",
    "        activation=\"paf\",\n",
    "        kernel_initializer=\"he_normal\"\n",
    "    ))\n",
    "\n",
    "model.compile(loss='mse', optimizer=keras.optimizers.Adam())\n",
    "\n",
    "# v. Training the model.\n",
    "print(\"Training run #\"+str(run_index))\n",
    "history = model.fit(X_train,\n",
    "                    Y_train,\n",
    "                    epochs=1000,\n",
    "                    validation_data=(X_valid, Y_valid),\n",
    "                    callbacks=[checkpoint_cb, tensorboard_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I. Loading the best parameters and generating a list of predictions.\n",
    "model = keras.models.load_model(model_name+\"_run_\"+str(run_index)+\".h5\")\n",
    "prediction = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# II. Visualizing the predictions.\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.tight_layout()\n",
    "plt.suptitle(\"Normalized decay factor vs time step\")\n",
    "for i in range(0, min(len(prediction), 9)):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.ylim(0.94,1.01)\n",
    "    plt.plot([x for x in prediction[i]])\n",
    "    plt.plot([x for x in Y_test[i]])\n",
    "plt.legend(['NN prediction','Actual decay'])\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,3))\n",
    "plt.suptitle(\"Normalized decay factor vs timestep: predicted (left) vs target (right)\")\n",
    "plt.subplot(1,2,1)\n",
    "plt.ylim(0.90,1)\n",
    "for i in range(0,len(prediction)):\n",
    "    plt.plot(prediction[i])\n",
    "plt.subplot(1,2,2)\n",
    "plt.ylim(0.90,1)\n",
    "for i in range(0,len(Y_test)):\n",
    "    plt.plot(Y_test[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IV. Computing the decay rates from the prediction and target, and\n",
    "#      computing the RMSE between those.\n",
    "model_pred = []\n",
    "degradation_rates_test = []\n",
    "for i in range(0,len(prediction)):\n",
    "    model_pred.append(extract_yr_degrad(prediction[i]))\n",
    "    degradation_rates_test.append(extract_yr_degrad(Y_test[i]))\n",
    "    \n",
    "plt.figure(figsize=(13,3))\n",
    "plt.plot(degradation_rates_test[0:50])\n",
    "plt.plot(model_pred[0:50])\n",
    "plt.xlabel(\"Time series sample #\")\n",
    "plt.ylabel(\"Yearly decay rate\")\n",
    "plt.title(\"NN prediction vs actual yearly decay rate\")\n",
    "plt.legend([\"Actual\",\"NN predicted\"])\n",
    "plt.show()\n",
    "    \n",
    "mse = metrics.mean_squared_error(model_pred, degradation_rates_test)\n",
    "print(\"Model's degradation RMSE: \"+str(round(math.sqrt(mse)*100, 5))+\"%/year\")\n",
    "print(\"Model's R2: \"+str(round(sklearn.metrics.r2_score(degradation_rates_test, model_pred), 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V. For subsampling: aggregating predictions from the 3-year-long subsamples\n",
    "#    coming from the same 5-year-long sample, and computing the RMSE.\n",
    "\n",
    "model_pred_aggr = []\n",
    "degrad_rate_aggr = []\n",
    "for i in range(len(model_pred)%3,len(model_pred),3):\n",
    "    degrad_rate_aggr.append(degradation_rates_test[i])\n",
    "    model_pred_aggr.append(np.mean(model_pred[i:i+3]))\n",
    "    \n",
    "plt.figure(figsize=(13,3))\n",
    "plt.plot(degrad_rate_aggr)\n",
    "plt.plot(model_pred_aggr)\n",
    "plt.xlabel(\"Time series sample #\")\n",
    "plt.ylabel(\"Yearly decay rate\")\n",
    "plt.title(\"NN prediction (via aggregation) vs actual yearly decay rate\")\n",
    "plt.legend([\"Actual\",\"NN predicted\"])\n",
    "plt.show()\n",
    "\n",
    "mse = metrics.mean_squared_error(model_pred_aggr, degrad_rate_aggr)\n",
    "print(\"Model's degradation RMSE: \"+str(round(math.sqrt(mse)*100, 5))+\"%/year\")\n",
    "print(\"Model's R2: \"+str(round(sklearn.metrics.r2_score(degrad_rate_aggr, model_pred_aggr), 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section is left empty for you to copy/paste models and preprocessing that work well. Too often did I want to try a modification and ended up editing and then forgetting about the edits of a promising model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
